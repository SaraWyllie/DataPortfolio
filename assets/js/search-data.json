{
  
    
        "post0": {
            "title": "Machine learning to identify types of glass.",
            "content": "For this project I will determine the optimal machine learning model for us in glassifying glass as one of 7 types based on its refractive index and composition. I will test logistic regression, k-nearest neighbors, decision tree, random forest, and support vector classifier models. The data for this project comes from the glass identification dataset from kaggle (https://www.kaggle.com/uciml/glass?select=glass.csv) . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . df=pd.read_csv(&#39;glass.csv&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 10 columns): RI 214 non-null float64 Na 214 non-null float64 Mg 214 non-null float64 Al 214 non-null float64 Si 214 non-null float64 K 214 non-null float64 Ca 214 non-null float64 Ba 214 non-null float64 Fe 214 non-null float64 Type 214 non-null int64 dtypes: float64(9), int64(1) memory usage: 16.8 KB . df.head() . RI Na Mg Al Si K Ca Ba Fe Type . 0 | 1.52101 | 13.64 | 4.49 | 1.10 | 71.78 | 0.06 | 8.75 | 0.0 | 0.0 | 1 | . 1 | 1.51761 | 13.89 | 3.60 | 1.36 | 72.73 | 0.48 | 7.83 | 0.0 | 0.0 | 1 | . 2 | 1.51618 | 13.53 | 3.55 | 1.54 | 72.99 | 0.39 | 7.78 | 0.0 | 0.0 | 1 | . 3 | 1.51766 | 13.21 | 3.69 | 1.29 | 72.61 | 0.57 | 8.22 | 0.0 | 0.0 | 1 | . 4 | 1.51742 | 13.27 | 3.62 | 1.24 | 73.08 | 0.55 | 8.07 | 0.0 | 0.0 | 1 | . sns.heatmap(df.isnull()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ee5165d780&gt; . Preprocessing . The features need to be scaled so the importance of large-scale features is not overestimated. . from sklearn.preprocessing import StandardScaler . sc=StandardScaler() . sc.fit(df.drop(&#39;Type&#39;, axis=1)) . StandardScaler(copy=True, with_mean=True, with_std=True) . sc_feats=sc.transform(df.drop(&#39;Type&#39;, axis=1)) sc_df=pd.DataFrame(sc_feats, columns=df.columns[:-1]) . sc_df.head() . RI Na Mg Al Si K Ca Ba Fe . 0 | 0.872868 | 0.284953 | 1.254639 | -0.692442 | -1.127082 | -0.671705 | -0.145766 | -0.352877 | -0.586451 | . 1 | -0.249333 | 0.591817 | 0.636168 | -0.170460 | 0.102319 | -0.026213 | -0.793734 | -0.352877 | -0.586451 | . 2 | -0.721318 | 0.149933 | 0.601422 | 0.190912 | 0.438787 | -0.164533 | -0.828949 | -0.352877 | -0.586451 | . 3 | -0.232831 | -0.242853 | 0.698710 | -0.310994 | -0.052974 | 0.112107 | -0.519052 | -0.352877 | -0.586451 | . 4 | -0.312045 | -0.169205 | 0.650066 | -0.411375 | 0.555256 | 0.081369 | -0.624699 | -0.352877 | -0.586451 | . Machine Learning Models . In order to determine an approriate model to use in classifying glass types I will test several different machine learning techniques and compare their accuracy. . X=sc_df y=df[&#39;Type&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) . Logistic Regression . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report from sklearn.metrics import accuracy_score . logreg=LogisticRegression() logreg.fit(X_train,y_train) lr_pred=logreg.predict(X_test) lr_score=accuracy_score(y_test,pred) . lrcm=(confusion_matrix(y_test, lr_pred)) . print(classification_report(y_test,lr_pred)) print(&#39; n&#39;) print(lr_score) . precision recall f1-score support 1 0.67 0.59 0.63 27 2 0.34 0.69 0.46 16 3 0.00 0.00 0.00 9 5 0.75 0.60 0.67 5 6 0.00 0.00 0.00 4 7 0.50 0.50 0.50 4 micro avg 0.49 0.49 0.49 65 macro avg 0.38 0.40 0.38 65 weighted avg 0.45 0.49 0.46 65 0.6615384615384615 . K-Nearest Neighbors . from sklearn.neighbors import KNeighborsClassifier . First, we need to determine the appropriate number of neighbors to use in the model. To do this, I check the error rate of k values between 1 and 20. . e_rate=[] for i in range(1,20): knn=KNeighborsClassifier(n_neighbors=i) knn.fit(X_train, y_train) pred_i=knn.predict(X_test) e_rate.append(np.mean(pred_i!=y_test)) . plt.figure(figsize=(10,6)) plt.plot(range(1,20), e_rate, color=&#39;b&#39;, linestyle=&#39;--&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;r&#39;, markersize=10) plt.title(&#39;Error-rate vs K value&#39;) plt.xlabel(&#39;K&#39;) plt.ylabel(&#39;Error-rate&#39;) . Text(0, 0.5, &#39;Error-rate&#39;) . Based on the error rates I chose to use 5 as the k value. At k=5, the error-rate is low and the and the error-rate curve does not show any extreme jumps. . knn=KNeighborsClassifier(n_neighbors=5) knn.fit(X_train, y_train) predK=knn.predict(X_test) . kcm=confusion_matrix(y_test, predK) k_score=accuracy_score(y_test, predK) k_score . 0.6615384615384615 . print(classification_report(y_test,predK)) print(&#39; n&#39;) print() . precision recall f1-score support 1 0.66 0.78 0.71 27 2 0.57 0.81 0.67 16 3 0.00 0.00 0.00 9 5 1.00 0.80 0.89 5 6 0.75 0.75 0.75 4 7 1.00 0.50 0.67 4 micro avg 0.66 0.66 0.66 65 macro avg 0.66 0.61 0.61 65 weighted avg 0.60 0.66 0.62 65 . Decision-Tree . from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier . dtree=DecisionTreeClassifier() dtree.fit(X_train, y_train) predT=dtree.predict(X_test) . dtcm=confusion_matrix(y_test,predT) . print(classification_report(y_test,predT)) print(accuracy_score(y_test,predT)) . precision recall f1-score support 1 0.78 0.78 0.78 27 2 0.50 0.81 0.62 16 3 0.67 0.22 0.33 9 5 1.00 0.60 0.75 5 6 0.67 0.50 0.57 4 7 0.67 0.50 0.57 4 micro avg 0.66 0.66 0.66 65 macro avg 0.71 0.57 0.60 65 weighted avg 0.70 0.66 0.65 65 0.6615384615384615 . Random Forest . rfc=RandomForestClassifier(n_estimators=1000) rfc.fit(X_train, y_train) predR=rfc.predict(X_test) . rfcm=confusion_matrix(y_test,predR) . print(classification_report(y_test,predR)) print(accuracy_score(y_test,predR)) . precision recall f1-score support 1 0.80 0.89 0.84 27 2 0.58 0.88 0.70 16 3 1.00 0.22 0.36 9 5 1.00 0.60 0.75 5 6 0.75 0.75 0.75 4 7 1.00 0.50 0.67 4 micro avg 0.74 0.74 0.74 65 macro avg 0.86 0.64 0.68 65 weighted avg 0.80 0.74 0.72 65 0.7384615384615385 . Support Vector Classifier . from sklearn.svm import SVC . sMod=SVC() sMod.fit(X_train, y_train) predS=sMod.predict(X_test) . svccm=confusion_matrix(y_test,predS) . print(classification_report(y_test,predS)) print(accuracy_score(y_test,predS)) . precision recall f1-score support 1 0.68 0.78 0.72 27 2 0.48 0.81 0.60 16 3 0.00 0.00 0.00 9 5 1.00 0.60 0.75 5 6 1.00 0.50 0.67 4 7 1.00 0.50 0.67 4 micro avg 0.63 0.63 0.63 65 macro avg 0.69 0.53 0.57 65 weighted avg 0.60 0.63 0.59 65 0.6307692307692307 . Conclusion . To evaluate the models we can examine the confusion matrices for each one. . cm_all=[lrcm, kcm, dtcm,rfcm,svccm] titles=[&#39;Logistic Regression&#39;, &#39;KNN&#39;, &#39;Decision Tree&#39;, &#39;Random Forest&#39;, &#39;SVC&#39;] #set up a counter for the subplots and titles count=1 #plot and label each confusion matrix plt.subplots(figsize=(12, 12)) plt.suptitle(&#39;Confusion Matrices&#39;) for i in cm_all: plt.subplot(3,2,count) sns.heatmap(i, annot=True,cbar=False, cmap=&#39;viridis&#39;) plt.title(titles[count-1]) count+=1 . scores=[accuracy_score(y_test,lr_pred),accuracy_score(y_test,predK), accuracy_score(y_test,predT), accuracy_score(y_test,predR),accuracy_score(y_test,predS)] . scoresDict = dict(zip(titles, scores)) sd=pd.DataFrame(scoresDict, index=[&#39;Accuracy&#39;]).round(3).transpose() sd[&#39;Accuracy&#39;]=sd[&#39;Accuracy&#39;].apply(lambda x:str(x*100)+&#39;%&#39;) . sd . Accuracy . Logistic Regression | 49.2% | . KNN | 66.2% | . Decision Tree | 66.2% | . Random Forest | 73.8% | . SVC | 63.1% | . The random forest method gives the highest degree of accuracy for this problem. It correctly predicts the glass type 73.8% of the time. The logistic regression model is the worst of the group with an accuracy of only 49.2%. .",
            "url": "https://sarawyllie.github.io/DataPortfolio/2021/08/24/glass.html",
            "relUrl": "/2021/08/24/glass.html",
            "date": " • Aug 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Analysis of Jeopardy Questions",
            "content": "The goal of this project is to examine trends in Jeopardy questions using the &#39;200,000+ Jeopardy! Questions&#39; data set (source: https://www.kaggle.com/tunguz/200000-jeopardy-questions). First I clean the data by correcting the column names, removing rows with missing values, and removing rows where with audio/video clues. Following that, I examine most used categories by round over the years the data spans. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.ticker as ticker import seaborn as sns %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, None) pd.set_option(&#39;display.max_columns&#39;, None) pd.options.display.width=None . df=pd.read_csv(&#39;JEOPARDY_CSV.csv&#39;) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 216930 entries, 0 to 216929 Data columns (total 7 columns): Show Number 216930 non-null int64 Air Date 216930 non-null object Round 216930 non-null object Category 216930 non-null object Value 216930 non-null object Question 216930 non-null object Answer 216928 non-null object dtypes: int64(1), object(6) memory usage: 11.6+ MB . data cleaning . df.columns = [x.strip() for x in df.columns] # remove dollar sign and commas from value column, convert none values to NaN, change all to integers df[&#39;Value&#39;] = df[&#39;Value&#39;].str.replace(&quot;$&quot;, &#39;&#39;) df[&#39;Value&#39;] = df[&#39;Value&#39;].str.replace(&quot;,&quot;, &#39;&#39;) df[&#39;Value&#39;] = df[&#39;Value&#39;].replace(&quot;None&quot;,np.nan).fillna(0).astype(int) . The show number does not provide any useful information so I drop that column. . df.drop(&#39;Show Number&#39;, inplace=True, axis=1) . Next I check for any rows containing null values. As there are only two such rows out of the 216,928 rows I simply drop them. I also check for any irregularities in the questions column and drop any rows where the question is listed as an audio or video clue or as filler, as they again make up a miniscule portion of the data. This will leave only rows that contain both questions and their answers. . df.isnull().sum() . Air Date 0 Round 0 Category 0 Value 0 Question 0 Answer 2 dtype: int64 . df[df[&#39;Answer&#39;].isnull()] . Air Date Round Category Value Question Answer . 94817 | 2003-06-23 | Jeopardy! | GOING &quot;N&quot;SANE | 200 | It often precedes &quot;and void&quot; | NaN | . 143297 | 2011-06-21 | Double Jeopardy! | NOTHING | 400 | This word for &quot;nothing&quot; precedes &quot;and void&quot; to... | NaN | . df.drop(df.index[[94817, 143297]], inplace=True) . df[&#39;Question&#39;].value_counts().head(10) . [audio clue] 17 [video clue] 14 (audio clue) 5 [filler] 5 Greenland 4 Hainan 4 Abigail Smith 4 &#34;A watched pot never&#34; does this 3 Howard 3 1861-1865 3 Name: Question, dtype: int64 . aud1=df[df[&#39;Question&#39;]==&#39;[audio clue]&#39;] aud2=df[df[&#39;Question&#39;]==&#39;(audio clue)&#39;] vid=df[df[&#39;Question&#39;]==&#39;[video clue]&#39;] filler=df[df[&#39;Question&#39;]==&#39;[filler]&#39;] df.drop(aud1.index, inplace=True) df.drop(aud2.index, inplace=True) df.drop(vid.index, inplace=True) df.drop(filler.index, inplace=True) . Finally I convert the date column to timeStamps and extract the year to its own column for convenience. . df[&#39;Air Date&#39;]=pd.to_datetime(df[&#39;Air Date&#39;], format=&#39;%Y-%m-%d&#39;) . def get_year (date_val): return (date_val).year df[&#39;Year&#39;]=df[&#39;Air Date&#39;].apply(get_year) . Plots . Now that the data is organized, I will examine the popularity of different categories. The first chart shows the 10 categories with the most questions from all years in the dataset for each of the possible rounds. For these years the choice of Jeopardy! questions. This shows that Before &amp; After was the most used category both overall and in the Double Jeopardy! round. . sns.set(rc={&#39;figure.figsize&#39;:(10,6)}) ax=sns.countplot(x=&#39;Category&#39;, data=df, hue=&#39;Round&#39;,order=df[&#39;Category&#39;].value_counts().head(10).index, palette=&#39;mako&#39;) ax.set_title(&#39;Most Common Categories by Round&#39;) ax.set_xlabel(&#39;Category&#39;) ax.set_ylabel(&#39;# of Appearances&#39;) plt.xticks(rotation=45, ha=&#39;right&#39;) plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) plt.tight_layout() . We now explore the categories with the most appearances in each of the two main rounds. In the Jeopardy! round we see a three-way tie between &#39;Stupid Answers&#39;, &#39;Potpourri&#39;, and &#39;Sports.&#39; The gap between the three most used and the 10th is 70 questions. . df_r1=df[df[&#39;Round&#39;]==&#39;Jeopardy!&#39;] df_r2=df[df[&#39;Round&#39;]==&#39;Double Jeopardy!&#39;] . df_r1[&#39;Category&#39;].value_counts().head(10) . POTPOURRI 255 STUPID ANSWERS 255 SPORTS 253 ANIMALS 233 AMERICAN HISTORY 227 SCIENCE 217 STATE CAPITALS 210 TELEVISION 200 U.S. CITIES 195 BUSINESS &amp; INDUSTRY 185 Name: Category, dtype: int64 . ax=sns.countplot(x=&#39;Category&#39;, data=df_r1, order=df_r1[&#39;Category&#39;].value_counts().head(10).index, color=&#39;b&#39;) ax.set_title(&#39;Jeopardy!&#39;) ax.set_xlabel(&#39;Category&#39;) ax.set_ylabel(&#39;# of Appearances&#39;) plt.xticks(rotation=45, ha=&#39;right&#39;) plt.tight_layout() . df_stupid=df[df[&#39;Category&#39;]==&#39;STUPID ANSWERS&#39;] df_bf=df[df[&#39;Category&#39;]==&#39;BEFORE &amp; AFTER&#39;] df_po=df[df[&#39;Category&#39;]==&#39;POTPOURRI&#39;] df_sp=df[df[&#39;Category&#39;]==&#39;SPORTS&#39;] . sns.set(rc={&#39;figure.figsize&#39;:(10,10)}) fig, ax=plt.subplots(3,1) ax1=sns.countplot(df_stupid[&#39;Year&#39;], ax=ax[0], color=&#39;b&#39;) ax2=sns.countplot(df_po[&#39;Year&#39;], ax=ax[1], color=&#39;r&#39;) ax3=sns.countplot(df_sp[&#39;Year&#39;], ax=ax[2], color=&#39;g&#39;) #for i in range(ax.shape[0]): # ax[i].xaxis.set_major_locator(plt.MaxNLocator(12)) ax1.set_ylabel(&#39;# of Appearances&#39;) ax2.set_ylabel(&#39;# of Appearances&#39;) ax3.set_ylabel(&#39;# of Appearances&#39;) ax1.set_title(&#39;Stupid Answers&#39;) ax2.set_title(&#39;Potpurri&#39;) ax3.set_title(&#39;Sports&#39;) plt.tight_layout(); . From these figures we see that the &#39;Sports&#39; category had the two highest yearly counts out of all three categories. These occured in 1997 and 1998 but otherwise was the least used of the three. This could be a greater interest in sports in those years. There were, for example, several sports scandals in 1997 (Mike Tyson, Dennis Rodman). &#39;Potpourri&#39; had the most consistent number of questions throughout the time period. It would seem that contestants should always be prepared for questions from this category. . Next, I plot the 10 most used categories for the time period in the Double Jeopardy! round. Here there is a difference of 200 questions between the most used, &#39;Before &amp; After,&#39; and the tenth most used. At first glance this would led one to believe that &#39;Before &amp; After&#39; is the most likely category to be encountered in a game of Jeopardy. However, let&#39;s look at how its popularity has changed over the time period. . df_r2[&#39;Category&#39;].value_counts().head() . BEFORE &amp; AFTER 450 LITERATURE 381 SCIENCE 296 WORLD GEOGRAPHY 254 OPERA 250 Name: Category, dtype: int64 . sns.set(rc={&#39;figure.figsize&#39;:(6,6)}) ax=sns.countplot(x=&#39;Category&#39;, data=df_r2, order=df_r2[&#39;Category&#39;].value_counts().head(10).index, color=&#39;r&#39;) ax.set_title(&#39;Double Jeopardy!&#39;) ax.set_xlabel(&#39;Category&#39;) ax.set_ylabel(&#39;# of Appearances&#39;) plt.xticks(rotation=45, ha=&#39;right&#39;) plt.tight_layout() . . Plotting the number of &#39;Before &amp; After&#39; questions by year shows a peak in 1999 but a consistent decline in usage from then until the end of the data collection period. While still a common category it would not have as much relevance in 2011 as it did in the few years surrounding 1999. . sns.set(rc={&#39;figure.figsize&#39;:(8,6)}) ax=sns.countplot(df_bf[&#39;Year&#39;], color=&#39;r&#39;) ax.set_ylabel(&#39;# of Appearances&#39;) ax.set_title(&#39;Before &amp; After&#39;) plt.tight_layout() .",
            "url": "https://sarawyllie.github.io/DataPortfolio/2021/08/18/Jeopardy.html",
            "relUrl": "/2021/08/18/Jeopardy.html",
            "date": " • Aug 18, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I recently received my PhD in physics. My graduate work was focused on quantum information and nonlinear optics. I worked extensively with Mathematica for theoretical research but also used Matlab, and Python for various projects, and contributed to machine learning projects in optical communication. I’ve been exploring various areas that the skills I learned in my physics education can be applied. This fostered an interest in data science. Here I will be dislaying data projects that I work on as I seek to expand my coding abilities. .",
          "url": "https://sarawyllie.github.io/DataPortfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sarawyllie.github.io/DataPortfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}