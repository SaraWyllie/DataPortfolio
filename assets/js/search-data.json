{
  
    
        "post0": {
            "title": "Data scraping and presentation with roller derby stats",
            "content": "As a roller derby player myself, it was only natural for me to look for roller derby statistics when trying to decide on a web scraping project. Stats are scraped from flattrackstats.com. First I will set scrape the site for multiple teams then do a closer examination of the stats for a particular team. . import requests from bs4 import BeautifulSoup import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set_style(&#39;whitegrid&#39;) pd.set_option(&#39;display.max_rows&#39;, None) pd.set_option(&#39;display.max_columns&#39;, None) pd.options.display.width=None import warnings warnings.filterwarnings(&#39;ignore&#39;) . Scraping the site . Flattrackstats has their statistics separated by team number. Each team page contains data table with bout statistics for many years. Each of these tables spans an additional four pages. I will two functions to perform the data scraping. The teamurl function takes in a particle team number and returns the url for that particular team&#39;s page. The derby scrape function loops through each team number and each team&#39;s pages. It uses pandas read_html method to scrape the pages and BeautifulSoup to parse them for team specific information. The raw data contains columns for home team names and visitor team names. In order to more easily analyze data by team, I would rather have columns that list the team name, the opponent, and whether the bout is home or away. To do that, I use BeautifulSoup to extract the team name for each team number and store those names in a new column. . def teamurl (team_no): base_site=&#39;http://flattrackstats.com/teams/&#39;+str(team_no) return base_site . def derby_scrape (pages,team_nums): #set up a datafframe for tables from each url and team names tabs=pd.DataFrame() name=pd.DataFrame() #loop through all desired team numbers for t in team_nums: baseurl=teamurl(t) #loop through each page the table spans for p in range(pages): url=baseurl+&#39;?page={0}&#39;.format(p) df=pd.read_html(url) #get data from statistics table (5th table on page), use BeautifulSoup to get the name for each team&#39;s number. st=df[5] response=requests.get(url) html=response.content soup=BeautifulSoup(html, &#39;lxml&#39;) st[&#39;Team&#39;]=soup.find(&#39;div&#39;, {&#39;class&#39;:&#39;leaguename&#39;}).text tabs=tabs.append(st) return tabs . rd_df=derby_scrape(4,list(range(3397,3402))) . rd_df.head() . Date Sanc. Home Team Score Visitor Team Score.1 Score Diff Tournament Bout Stats Team . 0 | 8/17/19 | NaN | Big Easy | 184.0 | Bux-Mont | 59.0 | 125.0 | NaN | Stats | Big Easy Rollergirls | . 1 | 7/27/19 | NaN | Big Easy | 283.0 | Carolina | 55.0 | 228.0 | NaN | Stats | Big Easy Rollergirls | . 2 | 6/22/19 | NaN | Big Easy | 156.0 | Rockin&#39; City | 82.0 | 74.0 | NaN | Stats | Big Easy Rollergirls | . 3 | 6/22/19 | NaN | North Texas | 98.0 | Big Easy | 132.0 | 34.0 | NaN | Stats | Big Easy Rollergirls | . 4 | 6/15/19 | NaN | Big Easy | 123.0 | Nashville | 160.0 | -37.0 | NaN | Stats | Big Easy Rollergirls | . Data Cleaning . rd_df.reset_index(inplace=True, drop=True) . sns.heatmap(rd_df.isnull()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x20c02c76b38&gt; . #score diff is dropped since it provides no new info #Bout stats column is dropped since it is just a series of links to mostly empty stat pages rd_df.drop([&#39;Sanc.&#39;, &#39;Score Diff&#39;, &#39;Bout Stats&#39;], axis=1, inplace=True) . I need to create a column that denotes whether a particular bout was home or away. To do that, I use a loop over all the bouts and compare the first word of the Team column with the first of the Home Team column. We cannot simply compare the two columns because the Team column gives the full team name while the Home Team column uses shortened versions. . home=[] for i in range(rd_df.shape[0]): tName=rd_df[&#39;Team&#39;][i].split() hTeam=rd_df[&#39;Home Team&#39;][i].split() if tName[0]==hTeam[0]: home.append(&#39;yes&#39;) else: home.append(&#39;no&#39;) rd_df[&#39;HomeBout&#39;]=home . #same for team&#39;s score and opponent&#39;s score Tscore=[] Oscore=[] Opponent=[] for i in range(rd_df.shape[0]): if rd_df[&#39;HomeBout&#39;][i]==&#39;yes&#39;: Opponent.append(rd_df[&#39;Visitor Team&#39;][i]) Tscore.append(rd_df[&#39;Score&#39;][i]) Oscore.append(rd_df[&#39;Score.1&#39;][i]) else: Opponent.append(rd_df[&#39;Home Team&#39;][i]) Tscore.append(rd_df[&#39;Score.1&#39;][i]) Oscore.append(rd_df[&#39;Score&#39;][i]) rd_df[&#39;Opponent&#39;]=Opponent rd_df[&#39;Team_score&#39;]=Tscore rd_df[&#39;Opponent_score&#39;]=Oscore . Next I will tackle the tournament column. The name of a particular tournament is not particularly useful but we may want to know whether a bout was part of a tournament or not. Tournaments generally involve multiple bouts played over at most a few days as opposed to non-tournament bouts that may take place only once a month. All of the null values can be replaced with &#39;no&#39; and the tournament names can be replaced with &#39;yes.&#39; . rd_df[&#39;Tournament&#39;].fillna(&#39;no&#39;, inplace=True) rd_df[&#39;Tournament&#39;]=np.where(rd_df[&#39;Tournament&#39;]!=&#39;no&#39;, &#39;yes&#39;, &#39;no&#39;) . df2=rd_df.copy() df2.head() . Date Home Team Score Visitor Team Score.1 Tournament Team HomeBout Opponent Team_score Opponent_score . 0 | 8/17/19 | Big Easy | 184.0 | Bux-Mont | 59.0 | no | Big Easy Rollergirls | yes | Bux-Mont | 184.0 | 59.0 | . 1 | 7/27/19 | Big Easy | 283.0 | Carolina | 55.0 | no | Big Easy Rollergirls | yes | Carolina | 283.0 | 55.0 | . 2 | 6/22/19 | Big Easy | 156.0 | Rockin&#39; City | 82.0 | no | Big Easy Rollergirls | yes | Rockin&#39; City | 156.0 | 82.0 | . 3 | 6/22/19 | North Texas | 98.0 | Big Easy | 132.0 | no | Big Easy Rollergirls | no | North Texas | 132.0 | 98.0 | . 4 | 6/15/19 | Big Easy | 123.0 | Nashville | 160.0 | no | Big Easy Rollergirls | yes | Nashville | 123.0 | 160.0 | . I&#39;d like to be able to look at yearly or monthly stats so I format the date column and extra the year and month to their own columns. . df2[&#39;Date&#39;]=pd.to_datetime(df2[&#39;Date&#39;]) df2[&#39;month&#39;] = pd.DatetimeIndex(df2[&#39;Date&#39;]).month df2[&#39;year&#39;]=pd.DatetimeIndex(df2[&#39;Date&#39;]).year df2[&#39;win&#39;]=np.where(df2[&#39;Score&#39;]&gt;df2[&#39;Score.1&#39;], &#39;win&#39;,&#39;loss&#39;) . Now there are quite a few columns that are no longer needed so I&#39;ll drop them and reorder my columns. . df2.drop([&#39;Home Team&#39;, &#39;Score&#39;, &#39;Visitor Team&#39;, &#39;Score.1&#39;], axis=1, inplace=True) . df2.columns . Index([&#39;Date&#39;, &#39;Tournament&#39;, &#39;Team&#39;, &#39;HomeBout&#39;, &#39;Opponent&#39;, &#39;Team_score&#39;, &#39;Opponent_score&#39;, &#39;month&#39;, &#39;year&#39;, &#39;win&#39;], dtype=&#39;object&#39;) . col_names=[&#39;Team&#39;, &#39;Opponent&#39;,&#39;Date&#39;, &#39;Team_score&#39;, &#39;Opponent_score&#39;,&#39;win&#39;, &#39;HomeBout&#39;, &#39;Tournament&#39;, &#39;month&#39;,&#39;year&#39;] df2=df2[col_names] . df2.head() . Team Opponent Date Team_score Opponent_score win HomeBout Tournament month year . 0 | Big Easy Rollergirls | Bux-Mont | 2019-08-17 | 184.0 | 59.0 | win | yes | no | 8 | 2019 | . 1 | Big Easy Rollergirls | Carolina | 2019-07-27 | 283.0 | 55.0 | win | yes | no | 7 | 2019 | . 2 | Big Easy Rollergirls | Rockin&#39; City | 2019-06-22 | 156.0 | 82.0 | win | yes | no | 6 | 2019 | . 3 | Big Easy Rollergirls | North Texas | 2019-06-22 | 132.0 | 98.0 | loss | no | no | 6 | 2019 | . 4 | Big Easy Rollergirls | Nashville | 2019-06-15 | 123.0 | 160.0 | loss | yes | no | 6 | 2019 | . Calculations and Visualizations . I&#39;d like to examine some stats for a particular team in more detail. For the sake of example, I&#39;ll use my team, Big Easy Rollergirls. . df=df2.copy() . sns.set(rc={&#39;figure.figsize&#39;:(10,6)}) sns.countplot(&#39;year&#39;, data=df[df[&#39;Team&#39;]==&#39;Big Easy Rollergirls&#39;], hue=&#39;win&#39;, palette=&#39;magma&#39;) plt.show() . sns.set(rc={&#39;figure.figsize&#39;:(6,4)}) sns.countplot(x=&#39;HomeBout&#39;, data=df[df[&#39;Team&#39;]==&#39;Big Easy Rollergirls&#39;], hue=&#39;win&#39;, palette=&#39;magma&#39;) plt.title(&#39;Total wins and losses for home and away bouts&#39;) plt.xlabel(&#39;Home Bout?&#39;) plt.ylabel(&#39;Total number of bouts&#39;) plt.show() . def win_rat (df, team): tdf=df[df[&#39;Team&#39;]==team] tot_bouts=tdf.groupby([&#39;year&#39;])[&#39;win&#39;].size() tot_wins=tdf[tdf[&#39;win&#39;]==&#39;win&#39;].groupby([&#39;year&#39;])[&#39;win&#39;].size() win_ratio=tot_wins/tot_bouts return win_ratio . def ppb (df, team): tdf=df[df[&#39;Team&#39;]==team] tot_bouts=tdf.groupby([&#39;year&#39;])[&#39;win&#39;].size() tot_points=tdf.groupby(&#39;year&#39;)[&#39;Team_score&#39;].sum() ppb=tot_points/tot_bouts return ppb . wr_be=win_rat(df,&#39;Big Easy Rollergirls&#39;) ppb_be=ppb(df, &#39;Big Easy Rollergirls&#39;) . fig, axes = plt.subplots(1, 2, figsize=(10,4)) years=list(range(2007,2020)) ax1=sns.regplot(y=wr_be, x=years, ax=axes[0], color=&#39;b&#39;) ax2=sns.regplot(y=ppb_be, x=years, ax=axes[1], color=&#39;b&#39;) ax1.set_xlim(2006,2019) ax2.set_xlim(2006,2019) ax1.set_title(&#39;Win Ratio by Year&#39;) ax1.set_xlabel(&#39;Year&#39;) ax1.set_ylabel(&#39;Win Ratio&#39;) ax2.set_title(&#39;Points per Bout by Year&#39;) ax2.set_xlabel(&#39;Year&#39;) ax2.set_ylabel(&#39;PPB&#39;) plt.suptitle(&#39;Big Easy Rollergirls Yearly Stats&#39;) plt.show() . Sadly, it would seem that our win ratio is decreasing slightly over time. Before we get too disappointed, I&#39;ll point out that their is a very large variance and the decrease since 2007 is small as can be seen in the slope of the trend line. It is more encouraging to see that our points per bout has been steadily increasing over time. . def wr_all (df, team): tdf=df[df[&#39;Team&#39;]==team] tot_bouts=tdf[&#39;win&#39;].shape[0] tot_wins=tdf[tdf[&#39;win&#39;]==&#39;win&#39;][&#39;win&#39;].shape[0] win_ratio=tot_wins/tot_bouts return win_ratio . wr=[] for t in df[&#39;Team&#39;].unique(): wr.append(wr_all(df, t)) . plt.figure(figsize=(10,6)) plt.bar(x=df[&#39;Team&#39;].unique(), height=wr) plt.xticks(rotation=20, ha=&#39;right&#39;) plt.ylim(bottom=.4) plt.axhline(y=0.5,linewidth=2, linestyle=&#39;--&#39;,color=&#39;k&#39;) plt.title(&#39;All-time win ratios&#39;) plt.xlabel(&#39;Teams&#39;) plt.ylabel(&#39;win ratio&#39;) plt.show() . This final plot shows the win ratio for each of the teams over the entire time period. The ratios above the dashed lines represent winning records (ie won at least 50% of their bouts). Of these five teams, the Dallas Derby Dolls have the best ll time win ratio. .",
            "url": "https://sarawyllie.github.io/DataPortfolio/2021/08/25/derby.html",
            "relUrl": "/2021/08/25/derby.html",
            "date": " • Aug 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Machine learning to identify types of glass.",
            "content": "For this project I will determine the optimal machine learning model for us in glassifying glass as one of 7 types based on its refractive index and composition. I will test logistic regression, k-nearest neighbors, decision tree, random forest, and support vector classifier models. The data for this project comes from the glass identification dataset from kaggle (https://www.kaggle.com/uciml/glass?select=glass.csv) . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . df=pd.read_csv(&#39;glass.csv&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 10 columns): RI 214 non-null float64 Na 214 non-null float64 Mg 214 non-null float64 Al 214 non-null float64 Si 214 non-null float64 K 214 non-null float64 Ca 214 non-null float64 Ba 214 non-null float64 Fe 214 non-null float64 Type 214 non-null int64 dtypes: float64(9), int64(1) memory usage: 16.8 KB . df.head() . RI Na Mg Al Si K Ca Ba Fe Type . 0 | 1.52101 | 13.64 | 4.49 | 1.10 | 71.78 | 0.06 | 8.75 | 0.0 | 0.0 | 1 | . 1 | 1.51761 | 13.89 | 3.60 | 1.36 | 72.73 | 0.48 | 7.83 | 0.0 | 0.0 | 1 | . 2 | 1.51618 | 13.53 | 3.55 | 1.54 | 72.99 | 0.39 | 7.78 | 0.0 | 0.0 | 1 | . 3 | 1.51766 | 13.21 | 3.69 | 1.29 | 72.61 | 0.57 | 8.22 | 0.0 | 0.0 | 1 | . 4 | 1.51742 | 13.27 | 3.62 | 1.24 | 73.08 | 0.55 | 8.07 | 0.0 | 0.0 | 1 | . sns.heatmap(df.isnull()) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x21f138feac8&gt; . Preprocessing . The features need to be scaled so the importance of large-scale features is not overestimated. . from sklearn.preprocessing import StandardScaler . sc=StandardScaler() . sc.fit(df.drop(&#39;Type&#39;, axis=1)) . StandardScaler(copy=True, with_mean=True, with_std=True) . sc_feats=sc.transform(df.drop(&#39;Type&#39;, axis=1)) sc_df=pd.DataFrame(sc_feats, columns=df.columns[:-1]) . sc_df.head() . RI Na Mg Al Si K Ca Ba Fe . 0 | 0.872868 | 0.284953 | 1.254639 | -0.692442 | -1.127082 | -0.671705 | -0.145766 | -0.352877 | -0.586451 | . 1 | -0.249333 | 0.591817 | 0.636168 | -0.170460 | 0.102319 | -0.026213 | -0.793734 | -0.352877 | -0.586451 | . 2 | -0.721318 | 0.149933 | 0.601422 | 0.190912 | 0.438787 | -0.164533 | -0.828949 | -0.352877 | -0.586451 | . 3 | -0.232831 | -0.242853 | 0.698710 | -0.310994 | -0.052974 | 0.112107 | -0.519052 | -0.352877 | -0.586451 | . 4 | -0.312045 | -0.169205 | 0.650066 | -0.411375 | 0.555256 | 0.081369 | -0.624699 | -0.352877 | -0.586451 | . Machine Learning Models . In order to determine an approriate model to use in glassifying glasss types I will test several different machine learning techniques and compare their accuracy. . Logistic Regression . from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report from sklearn.metrics import accuracy_score . X=sc_df y=df[&#39;Type&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) . logreg=LogisticRegression() logreg.fit(X_train,y_train) lr_pred=logreg.predict(X_test) lr_score=accuracy_score(y_test,lr_pred) . lrcm=(confusion_matrix(y_test, lr_pred)) . print(classification_report(y_test,lr_pred)) print(&#39; n&#39;) print(lr_score) . precision recall f1-score support 1 0.67 0.59 0.63 27 2 0.34 0.69 0.46 16 3 0.00 0.00 0.00 9 5 0.75 0.60 0.67 5 6 0.00 0.00 0.00 4 7 0.50 0.50 0.50 4 micro avg 0.49 0.49 0.49 65 macro avg 0.38 0.40 0.38 65 weighted avg 0.45 0.49 0.46 65 0.49230769230769234 . K-Nearest Neighbors . from sklearn.neighbors import KNeighborsClassifier . First, we need to determine the appropriate number of neighbors to use in the model. To do this, I check the error rate of k values between 1 and 20. . e_rate=[] for i in range(1,20): knn=KNeighborsClassifier(n_neighbors=i) knn.fit(X_train, y_train) pred_i=knn.predict(X_test) e_rate.append(np.mean(pred_i!=y_test)) . plt.figure(figsize=(10,6)) plt.plot(range(1,20), e_rate, color=&#39;b&#39;, linestyle=&#39;--&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;r&#39;, markersize=10) plt.title(&#39;Error-rate vs K value&#39;) plt.xlabel(&#39;K&#39;) plt.ylabel(&#39;Error-rate&#39;) . Text(0, 0.5, &#39;Error-rate&#39;) . Based on the error rates I chose to use 5 as the k value. At k=5, the error-rate is low and the and the error-rate curve does not show any extreme jumps. . knn=KNeighborsClassifier(n_neighbors=5) knn.fit(X_train, y_train) predK=knn.predict(X_test) . kcm=confusion_matrix(y_test, predK) k_score=accuracy_score(y_test, predK) k_score . 0.6615384615384615 . print(classification_report(y_test,predK)) print(&#39; n&#39;) print() . precision recall f1-score support 1 0.66 0.78 0.71 27 2 0.57 0.81 0.67 16 3 0.00 0.00 0.00 9 5 1.00 0.80 0.89 5 6 0.75 0.75 0.75 4 7 1.00 0.50 0.67 4 micro avg 0.66 0.66 0.66 65 macro avg 0.66 0.61 0.61 65 weighted avg 0.60 0.66 0.62 65 . Decision-Tree . from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier . dtree=DecisionTreeClassifier() dtree.fit(X_train, y_train) predT=dtree.predict(X_test) . dtcm=confusion_matrix(y_test,predT) . print(classification_report(y_test,predT)) print(accuracy_score(y_test,predT)) . precision recall f1-score support 1 0.82 0.67 0.73 27 2 0.54 0.81 0.65 16 3 0.40 0.44 0.42 9 5 1.00 0.60 0.75 5 6 0.67 0.50 0.57 4 7 0.67 0.50 0.57 4 micro avg 0.65 0.65 0.65 65 macro avg 0.68 0.59 0.62 65 weighted avg 0.69 0.65 0.65 65 0.6461538461538462 . Random Forest . rfc=RandomForestClassifier(n_estimators=1000) rfc.fit(X_train, y_train) predR=rfc.predict(X_test) . rfcm=confusion_matrix(y_test,predR) . print(classification_report(y_test,predR)) print(accuracy_score(y_test,predR)) . precision recall f1-score support 1 0.77 0.89 0.83 27 2 0.61 0.88 0.72 16 3 1.00 0.22 0.36 9 5 1.00 0.60 0.75 5 6 0.75 0.75 0.75 4 7 1.00 0.50 0.67 4 micro avg 0.74 0.74 0.74 65 macro avg 0.86 0.64 0.68 65 weighted avg 0.79 0.74 0.72 65 0.7384615384615385 . Support Vector Classifier . from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV . sMod=SVC() sMod.fit(X_train, y_train) predS=sMod.predict(X_test) . svccm=confusion_matrix(y_test,predS) . print(classification_report(y_test,predS)) print(accuracy_score(y_test,predS)) . precision recall f1-score support 1 0.68 0.78 0.72 27 2 0.48 0.81 0.60 16 3 0.00 0.00 0.00 9 5 1.00 0.60 0.75 5 6 1.00 0.50 0.67 4 7 1.00 0.50 0.67 4 micro avg 0.63 0.63 0.63 65 macro avg 0.69 0.53 0.57 65 weighted avg 0.60 0.63 0.59 65 0.6307692307692307 . Since this SVC model used the default parameters, I will use a grid search to find the optimal parameters in the hopes of improving the accuracy. . param_grid={&#39;C&#39;:[.1,1,10,100,1000], &#39;gamma&#39;:[1,.1,.01,.001,.0001]} . grid=GridSearchCV(SVC(), param_grid) . grid.fit(X_train, y_train) . C: Users sarar Anaconda lib site-packages sklearn model_selection _search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal. DeprecationWarning) . GridSearchCV(cv=&#39;warn&#39;, error_score=&#39;raise-deprecating&#39;, estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid={&#39;C&#39;: [0.1, 1, 10, 100, 1000], &#39;gamma&#39;: [1, 0.1, 0.01, 0.001, 0.0001]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=0) . grid_pred=grid.predict(X_test) . print(classification_report(y_test,grid_pred)) print(accuracy_score(y_test,grid_pred)) . precision recall f1-score support 1 0.71 0.89 0.79 27 2 0.65 0.81 0.72 16 3 0.00 0.00 0.00 9 5 0.67 0.80 0.73 5 6 1.00 0.75 0.86 4 7 1.00 0.50 0.67 4 micro avg 0.71 0.71 0.71 65 macro avg 0.67 0.63 0.63 65 weighted avg 0.63 0.71 0.65 65 0.7076923076923077 . svccm=confusion_matrix(y_test, grid_pred) . Conclusion . To evaluate the models we can examine the confusion matrices for each one. . cm_all=[lrcm, kcm, dtcm,rfcm,svccm] titles=[&#39;Logistic Regression&#39;, &#39;KNN&#39;, &#39;Decision Tree&#39;, &#39;Random Forest&#39;, &#39;SVC&#39;] #set up a counter for the subplots and titles count=1 #plot and label each confusion matrix plt.subplots(figsize=(12, 12)) plt.suptitle(&#39;Confusion Matrices&#39;) for i in cm_all: plt.subplot(3,2,count) sns.heatmap(i, annot=True,cbar=False, cmap=&#39;viridis&#39;) plt.title(titles[count-1]) count+=1 . scores=[accuracy_score(y_test,lr_pred),accuracy_score(y_test,predK), accuracy_score(y_test,predT), accuracy_score(y_test,predR),accuracy_score(y_test,grid_pred)] . scoresDict = dict(zip(titles, scores)) sd=pd.DataFrame(scoresDict, index=[&#39;Accuracy&#39;]).round(3).transpose() sd[&#39;Accuracy&#39;]=sd[&#39;Accuracy&#39;].apply(lambda x:str(x*100)+&#39;%&#39;) . sd . Accuracy . Logistic Regression | 49.2% | . KNN | 66.2% | . Decision Tree | 64.60000000000001% | . Random Forest | 73.8% | . SVC | 70.8% | . The random forest method gives the highest degree of accuracy for this problem. It correctly predicts the glass type 73.8% of the time. The logistic regression model is the worst of the group with an accuracy of only 49.2%. .",
            "url": "https://sarawyllie.github.io/DataPortfolio/2021/08/24/glass.html",
            "relUrl": "/2021/08/24/glass.html",
            "date": " • Aug 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Data Analysis of Jeopardy Questions",
            "content": "The goal of this project is to examine trends in Jeopardy questions using the &#39;200,000+ Jeopardy! Questions&#39; data set (source: https://www.kaggle.com/tunguz/200000-jeopardy-questions). First I clean the data by correcting the column names, removing rows with missing values, and removing rows where with audio/video clues. Following that, I examine most used categories by round over the years the data spans. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.ticker as ticker import seaborn as sns %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, None) pd.set_option(&#39;display.max_columns&#39;, None) pd.options.display.width=None . df=pd.read_csv(&#39;JEOPARDY_CSV.csv&#39;) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 216930 entries, 0 to 216929 Data columns (total 7 columns): Show Number 216930 non-null int64 Air Date 216930 non-null object Round 216930 non-null object Category 216930 non-null object Value 216930 non-null object Question 216930 non-null object Answer 216928 non-null object dtypes: int64(1), object(6) memory usage: 11.6+ MB . data cleaning . df.columns = [x.strip() for x in df.columns] # remove dollar sign and commas from value column, convert none values to NaN, change all to integers df[&#39;Value&#39;] = df[&#39;Value&#39;].str.replace(&quot;$&quot;, &#39;&#39;) df[&#39;Value&#39;] = df[&#39;Value&#39;].str.replace(&quot;,&quot;, &#39;&#39;) df[&#39;Value&#39;] = df[&#39;Value&#39;].replace(&quot;None&quot;,np.nan).fillna(0).astype(int) . The show number does not provide any useful information so I drop that column. . df.drop(&#39;Show Number&#39;, inplace=True, axis=1) . Next I check for any rows containing null values. As there are only two such rows out of the 216,928 rows I simply drop them. I also check for any irregularities in the questions column and drop any rows where the question is listed as an audio or video clue or as filler, as they again make up a miniscule portion of the data. This will leave only rows that contain both questions and their answers. . df.isnull().sum() . Air Date 0 Round 0 Category 0 Value 0 Question 0 Answer 2 dtype: int64 . df[df[&#39;Answer&#39;].isnull()] . Air Date Round Category Value Question Answer . 94817 | 2003-06-23 | Jeopardy! | GOING &quot;N&quot;SANE | 200 | It often precedes &quot;and void&quot; | NaN | . 143297 | 2011-06-21 | Double Jeopardy! | NOTHING | 400 | This word for &quot;nothing&quot; precedes &quot;and void&quot; to... | NaN | . df.drop(df.index[[94817, 143297]], inplace=True) . df[&#39;Question&#39;].value_counts().head(10) . [audio clue] 17 [video clue] 14 (audio clue) 5 [filler] 5 Greenland 4 Hainan 4 Abigail Smith 4 &#34;A watched pot never&#34; does this 3 Howard 3 1861-1865 3 Name: Question, dtype: int64 . aud1=df[df[&#39;Question&#39;]==&#39;[audio clue]&#39;] aud2=df[df[&#39;Question&#39;]==&#39;(audio clue)&#39;] vid=df[df[&#39;Question&#39;]==&#39;[video clue]&#39;] filler=df[df[&#39;Question&#39;]==&#39;[filler]&#39;] df.drop(aud1.index, inplace=True) df.drop(aud2.index, inplace=True) df.drop(vid.index, inplace=True) df.drop(filler.index, inplace=True) . Finally I convert the date column to timeStamps and extract the year to its own column for convenience. . df[&#39;Air Date&#39;]=pd.to_datetime(df[&#39;Air Date&#39;], format=&#39;%Y-%m-%d&#39;) . def get_year (date_val): return (date_val).year df[&#39;Year&#39;]=df[&#39;Air Date&#39;].apply(get_year) . Plots . Now that the data is organized, I will examine the popularity of different categories. The first chart shows the 10 categories with the most questions from all years in the dataset for each of the possible rounds. For these years the choice of Jeopardy! questions. This shows that Before &amp; After was the most used category both overall and in the Double Jeopardy! round. . sns.set(rc={&#39;figure.figsize&#39;:(10,6)}) ax=sns.countplot(x=&#39;Category&#39;, data=df, hue=&#39;Round&#39;,order=df[&#39;Category&#39;].value_counts().head(10).index, palette=&#39;mako&#39;) ax.set_title(&#39;Most Common Categories by Round&#39;) ax.set_xlabel(&#39;Category&#39;) ax.set_ylabel(&#39;# of Appearances&#39;) plt.xticks(rotation=45, ha=&#39;right&#39;) plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) plt.tight_layout() . We now explore the categories with the most appearances in each of the two main rounds. In the Jeopardy! round we see a three-way tie between &#39;Stupid Answers&#39;, &#39;Potpourri&#39;, and &#39;Sports.&#39; The gap between the three most used and the 10th is 70 questions. . df_r1=df[df[&#39;Round&#39;]==&#39;Jeopardy!&#39;] df_r2=df[df[&#39;Round&#39;]==&#39;Double Jeopardy!&#39;] . df_r1[&#39;Category&#39;].value_counts().head(10) . POTPOURRI 255 STUPID ANSWERS 255 SPORTS 253 ANIMALS 233 AMERICAN HISTORY 227 SCIENCE 217 STATE CAPITALS 210 TELEVISION 200 U.S. CITIES 195 BUSINESS &amp; INDUSTRY 185 Name: Category, dtype: int64 . ax=sns.countplot(x=&#39;Category&#39;, data=df_r1, order=df_r1[&#39;Category&#39;].value_counts().head(10).index, color=&#39;b&#39;) ax.set_title(&#39;Jeopardy!&#39;) ax.set_xlabel(&#39;Category&#39;) ax.set_ylabel(&#39;# of Appearances&#39;) plt.xticks(rotation=45, ha=&#39;right&#39;) plt.tight_layout() . df_stupid=df[df[&#39;Category&#39;]==&#39;STUPID ANSWERS&#39;] df_bf=df[df[&#39;Category&#39;]==&#39;BEFORE &amp; AFTER&#39;] df_po=df[df[&#39;Category&#39;]==&#39;POTPOURRI&#39;] df_sp=df[df[&#39;Category&#39;]==&#39;SPORTS&#39;] . sns.set(rc={&#39;figure.figsize&#39;:(10,10)}) fig, ax=plt.subplots(3,1) ax1=sns.countplot(df_stupid[&#39;Year&#39;], ax=ax[0], color=&#39;b&#39;) ax2=sns.countplot(df_po[&#39;Year&#39;], ax=ax[1], color=&#39;r&#39;) ax3=sns.countplot(df_sp[&#39;Year&#39;], ax=ax[2], color=&#39;g&#39;) #for i in range(ax.shape[0]): # ax[i].xaxis.set_major_locator(plt.MaxNLocator(12)) ax1.set_ylabel(&#39;# of Appearances&#39;) ax2.set_ylabel(&#39;# of Appearances&#39;) ax3.set_ylabel(&#39;# of Appearances&#39;) ax1.set_title(&#39;Stupid Answers&#39;) ax2.set_title(&#39;Potpurri&#39;) ax3.set_title(&#39;Sports&#39;) plt.tight_layout(); . From these figures we see that the &#39;Sports&#39; category had the two highest yearly counts out of all three categories. These occured in 1997 and 1998 but otherwise was the least used of the three. This could be a greater interest in sports in those years. There were, for example, several sports scandals in 1997 (Mike Tyson, Dennis Rodman). &#39;Potpourri&#39; had the most consistent number of questions throughout the time period. It would seem that contestants should always be prepared for questions from this category. . Next, I plot the 10 most used categories for the time period in the Double Jeopardy! round. Here there is a difference of 200 questions between the most used, &#39;Before &amp; After,&#39; and the tenth most used. At first glance this would led one to believe that &#39;Before &amp; After&#39; is the most likely category to be encountered in a game of Jeopardy. However, let&#39;s look at how its popularity has changed over the time period. . df_r2[&#39;Category&#39;].value_counts().head() . BEFORE &amp; AFTER 450 LITERATURE 381 SCIENCE 296 WORLD GEOGRAPHY 254 OPERA 250 Name: Category, dtype: int64 . sns.set(rc={&#39;figure.figsize&#39;:(6,6)}) ax=sns.countplot(x=&#39;Category&#39;, data=df_r2, order=df_r2[&#39;Category&#39;].value_counts().head(10).index, color=&#39;r&#39;) ax.set_title(&#39;Double Jeopardy!&#39;) ax.set_xlabel(&#39;Category&#39;) ax.set_ylabel(&#39;# of Appearances&#39;) plt.xticks(rotation=45, ha=&#39;right&#39;) plt.tight_layout() . . Plotting the number of &#39;Before &amp; After&#39; questions by year shows a peak in 1999 but a consistent decline in usage from then until the end of the data collection period. While still a common category it would not have as much relevance in 2011 as it did in the few years surrounding 1999. . sns.set(rc={&#39;figure.figsize&#39;:(8,6)}) ax=sns.countplot(df_bf[&#39;Year&#39;], color=&#39;r&#39;) ax.set_ylabel(&#39;# of Appearances&#39;) ax.set_title(&#39;Before &amp; After&#39;) plt.tight_layout() .",
            "url": "https://sarawyllie.github.io/DataPortfolio/2021/08/18/Jeopardy.html",
            "relUrl": "/2021/08/18/Jeopardy.html",
            "date": " • Aug 18, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I recently received my PhD in physics. My graduate work was focused on quantum information and nonlinear optics. I worked extensively with Mathematica for theoretical research but also used Matlab, and Python for various projects, and contributed to machine learning projects in optical communication. I’ve been exploring various areas that the skills I learned in my physics education can be applied. This fostered an interest in data science. Here I will be dislaying data projects that I work on as I seek to expand my coding abilities. .",
          "url": "https://sarawyllie.github.io/DataPortfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sarawyllie.github.io/DataPortfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}